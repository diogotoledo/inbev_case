# üç∫ BEES Data Engineering Case ‚Äî InBev

Pipeline de dados seguindo a arquitetura Medallion **(Bronze ‚Üí Silver ‚Üí Gold)**,
orquestrado com **Apache Airflow** e containerizado com **Docker Compose**.

## Arquitetura

```
Open Brewery DB API
        ‚îÇ
   [Bronze Layer]  ‚îÄ‚îÄ JSON raw salvo com timestamp
        ‚îÇ
   [Silver Layer]  ‚îÄ‚îÄ Parquet particionado por pa√≠s / estado
        ‚îÇ
    [Gold Layer]   ‚îÄ‚îÄ Parquet agregado por tipo de cervejaria + localidade
        ‚îÇ
 [Data Quality]   ‚îÄ‚îÄ Valida√ß√µes autom√°ticas via Airflow task
```

## Estrutura do Projeto

```
inbev-case/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ brewery_pipeline.py       # DAG principal
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/ingest_api.py      # Ingest√£o da API
‚îÇ   ‚îú‚îÄ‚îÄ silver/transform.py       # Transforma√ß√£o ‚Üí Parquet particionado
‚îÇ   ‚îî‚îÄ‚îÄ gold/aggregate.py         # Agrega√ß√£o por tipo e localiza√ß√£o
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_ingest.py
‚îÇ   ‚îî‚îÄ‚îÄ test_transform.py
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_bronze_exploration.ipynb   # Explora√ß√£o da camada raw
‚îÇ   ‚îú‚îÄ‚îÄ 02_silver_exploration.ipynb   # Explora√ß√£o da camada silver
‚îÇ   ‚îî‚îÄ‚îÄ 03_gold_exploration.ipynb     # Explora√ß√£o da camada gold
‚îú‚îÄ‚îÄ data/                         # Data lake local (bronze/silver/gold)
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt              # Depend√™ncias do pipeline (Docker)
‚îú‚îÄ‚îÄ requirements-dev.txt          # Depend√™ncias de desenvolvimento local
‚îî‚îÄ‚îÄ .env
```

## Como executar

### Pr√©-requisitos
- Docker Desktop instalado e rodando

### 1. Clone o reposit√≥rio
```bash
git clone https://github.com/diogotoledo/inbev_case
cd inbev-case
```

### 2. Suba os containers
```bash
docker-compose up --build
```
Aguarde ~2 minutos na primeira execu√ß√£o.

### 3. Acesse o Airflow
- URL: http://localhost:8080
- Usu√°rio: `admin` | Senha: `admin`

### 4. Execute o pipeline
- Ative a DAG **brewery_pipeline**
- Clique em **Trigger DAG ‚ñ∂**

### 5. Rode os testes
```bash
docker-compose exec airflow-scheduler pytest tests/ -v
```

## An√°lise Explorat√≥ria (opcional)

Esta an√°lise foi inclu√≠da por quest√µes de visualiza√ß√£o somente, me considero uma pessoa muito visual,
ent√£o gosto de visualizar os dados em tabelas para estabelecer algumas rela√ß√µes inerentes ao pipeline.

Os notebooks de explora√ß√£o rodam **localmente**, fora do Docker.
Instale as depend√™ncias de desenvolvimento e suba o Jupyter:

```bash
# Instalar depend√™ncias de desenvolvimento (inclui Jupyter)
pip install -r requirements-dev.txt

# Subir o Jupyter Notebook
jupyter notebook
```

Acesse a pasta `notebooks/` e abra os notebooks na ordem:

|        Notebook         |   Camada   |                     Conte√∫do                               |
|-------------------------|------------|------------------------------------------------------------|
| `01_bronze_exploration` | üü´ Bronze | Dados brutos da API, nulos, distribui√ß√£o por pa√≠s/tipo      |
| `02_silver_exploration` | ü•à Silver | Parti√ß√µes Parquet, tipos de dados, coordenadas geogr√°ficas  |
| `03_gold_exploration`   | ü•á Gold   | Agrega√ß√µes por tipo/pa√≠s/estado, pivot tables, data quality |

> **Nota:** os notebooks usam caminhos relativos (`../data/`), portanto devem
> ser executados a partir da pasta `notebooks/`.

## Design Choices

|       Decis√£o       |               Escolha                   |                            Motivo                                |
|---------------------|-----------------------------------------|------------------------------------------------------------------|
| Orquestra√ß√£o        | Apache Airflow                          | Padr√£o enterprise, retry e scheduling nativos                    |
| Formato silver/gold | Parquet                                 | Columnar, eficiente para leitura anal√≠tica                       |
| Particionamento     | country + state                         | Otimiza queries por localiza√ß√£o                                  |
| Containeriza√ß√£o     | Docker Compose                          | Ambiente 100% reproduz√≠vel                                       |
| Monitoramento       | Data Quality Task no Airflow            | Detecta dados inv√°lidos ou ausentes                              |
| Depend√™ncias        | requirements.txt + requirements-dev.txt | Separa depend√™ncias de runtime das de desenvolvimento            |

## Monitoramento e Alertas

- **Retries autom√°ticos**: 3 tentativas com intervalo de 5 minutos por task
- **Data Quality Task**: valida volume, nulos e integridade do gold layer ap√≥s cada run
- **Logs centralizados**: dispon√≠veis na UI do Airflow por task e execu√ß√£o
- **Extens√£o sugerida**: Airflow Connections com Slack/e-mail para alertas em falha de produ√ß√£o

## Limita√ß√µes conhecidas da fonte de dados

A **Open Brewery DB** √© um dataset open-source mantido por contribui√ß√µes volunt√°rias
da comunidade via Pull Requests no GitHub. Por esse motivo, o dataset √© composto
principalmente por pa√≠ses de l√≠ngua inglesa, sendo os seguintes os pa√≠ses atualmente dispon√≠veis:

> Australia, Austria, Canada, England, France, Germany, Ireland, Isle of Man, Italy,
> Japan, Poland, Portugal, Scotland, Singapore, South Africa, South Korea, Sweden,
> Ukraine e United States.

**Pa√≠ses como Brasil n√£o est√£o presentes na fonte de dados.** Isso n√£o √© uma limita√ß√£o
do pipeline, mas sim da origem dos dados. O pipeline processa corretamente todos os
registros disponibilizados pela API.

### Extens√£o sugerida para outras fontes

Para cobrir mercados n√£o presentes na Open Brewery DB (como o Brasil), o pipeline
poderia ser estendido para ingerir dados de fontes complementares, como:

- **Untappd API** ‚Äî base de dados global de cervejas e cervejarias
- **RateBeer API** ‚Äî cat√°logo global com cobertura de pa√≠ses da Am√©rica Latina
- **Scraping de associa√ß√µes locais** ‚Äî ex: CervBrasil (Associa√ß√£o Brasileira de Cerveja Artesanal)

A arquitetura Medallion adotada facilita essa extens√£o: bastaria adicionar novos
operadores na camada Bronze para cada fonte adicional, mantendo as camadas Silver
e Gold agn√≥sticas √† origem dos dados.
