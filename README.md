# ğŸº BEES Data Engineering Case â€” InBev

Pipeline de dados seguindo a arquitetura Medallion **(Bronze â†’ Silver â†’ Gold)**,
orquestrado com **Apache Airflow** e containerizado com **Docker Compose**.

## Arquitetura

```
Open Brewery DB API
        â”‚
   [Bronze Layer]  â”€â”€ JSON raw salvo com timestamp
        â”‚
   [Silver Layer]  â”€â”€ Parquet particionado por paÃ­s / estado
        â”‚
    [Gold Layer]   â”€â”€ Parquet agregado por tipo de cervejaria + localidade
        â”‚
 [Data Quality]   â”€â”€ ValidaÃ§Ãµes automÃ¡ticas via Airflow task
```

## Estrutura do Projeto

```
inbev-case/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ brewery_pipeline.py       # DAG principal
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bronze/ingest_api.py      # IngestÃ£o da API
â”‚   â”œâ”€â”€ silver/transform.py       # TransformaÃ§Ã£o â†’ Parquet particionado
â”‚   â””â”€â”€ gold/aggregate.py         # AgregaÃ§Ã£o por tipo e localizaÃ§Ã£o
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingest.py
â”‚   â””â”€â”€ test_transform.py
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_bronze_exploration.ipynb   # ExploraÃ§Ã£o da camada raw
â”‚   â”œâ”€â”€ 02_silver_exploration.ipynb   # ExploraÃ§Ã£o da camada silver
â”‚   â””â”€â”€ 03_gold_exploration.ipynb     # ExploraÃ§Ã£o da camada gold
â”œâ”€â”€ data/                         # Data lake local (bronze/silver/gold)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt              # DependÃªncias do pipeline (Docker)
â”œâ”€â”€ requirements-dev.txt          # DependÃªncias de desenvolvimento local
â””â”€â”€ .env
```

## Como executar

### PrÃ©-requisitos
- Docker Desktop instalado e rodando

### 1. Clone o repositÃ³rio
```bash
git clone [<repo-url>](https://github.com/diogotoledo/inbev_case)
cd inbev-case
```

### 2. Suba os containers
```bash
docker-compose up --build
```
Aguarde ~2 minutos na primeira execuÃ§Ã£o.

### 3. Acesse o Airflow
- URL: http://localhost:8080
- UsuÃ¡rio: `admin` | Senha: `admin`

### 4. Execute o pipeline
- Ative a DAG **brewery_pipeline**
- Clique em **Trigger DAG â–¶**

### 5. Rode os testes
```bash
docker-compose exec airflow-scheduler pytest tests/ -v
```

## AnÃ¡lise ExploratÃ³ria (opcional)

Esta anÃ¡lise foi incluÃ­da por questÃµes de visualizaÃ§Ã£o somente, me considero uma pessoa muito visual,
entÃ£o gosto de visualizar os dados em tabelas para estabelecer algumas relaÃ§Ãµes inerentes ao pipeline.

Os notebooks de exploraÃ§Ã£o rodam **localmente**, fora do Docker.
Instale as dependÃªncias de desenvolvimento e suba o Jupyter:

```bash
# Instalar dependÃªncias de desenvolvimento (inclui Jupyter)
pip install -r requirements-dev.txt

# Subir o Jupyter Notebook
jupyter notebook
```

Acesse a pasta `notebooks/` e abra os notebooks na ordem:

|        Notebook         |   Camada   |                     ConteÃºdo                               |
|-------------------------|------------|------------------------------------------------------------|
| `01_bronze_exploration` | ğŸŸ« Bronze | Dados brutos da API, nulos, distribuiÃ§Ã£o por paÃ­s/tipo      |
| `02_silver_exploration` | ğŸ¥ˆ Silver | PartiÃ§Ãµes Parquet, tipos de dados, coordenadas geogrÃ¡ficas  |
| `03_gold_exploration`   | ğŸ¥‡ Gold   | AgregaÃ§Ãµes por tipo/paÃ­s/estado, pivot tables, data quality |

> **Nota:** os notebooks usam caminhos relativos (`../data/`), portanto devem
> ser executados a partir da pasta `notebooks/`.

## Design Choices

|       DecisÃ£o       |               Escolha                   |                            Motivo                                |
|---------------------|-----------------------------------------|------------------------------------------------------------------|
| OrquestraÃ§Ã£o        | Apache Airflow                          | PadrÃ£o enterprise, retry e scheduling nativos                    |
| Formato silver/gold | Parquet                                 | Columnar, eficiente para leitura analÃ­tica                       |
| Particionamento     | country + state                         | Otimiza queries por localizaÃ§Ã£o                                  |
| ContainerizaÃ§Ã£o     | Docker Compose                          | Ambiente 100% reproduzÃ­vel                                       |
| Monitoramento       | Data Quality Task no Airflow            | Detecta dados invÃ¡lidos ou ausentes                              |
| DependÃªncias        | requirements.txt + requirements-dev.txt | Separa dependÃªncias de runtime das de desenvolvimento            |

## Monitoramento e Alertas

- **Retries automÃ¡ticos**: 3 tentativas com intervalo de 5 minutos por task
- **Data Quality Task**: valida volume, nulos e integridade do gold layer apÃ³s cada run
- **Logs centralizados**: disponÃ­veis na UI do Airflow por task e execuÃ§Ã£o
- **ExtensÃ£o sugerida**: Airflow Connections com Slack/e-mail para alertas em falha de produÃ§Ã£o

## LimitaÃ§Ãµes conhecidas da fonte de dados

A **Open Brewery DB** Ã© um dataset open-source mantido por contribuiÃ§Ãµes voluntÃ¡rias
da comunidade via Pull Requests no GitHub. Por esse motivo, o dataset Ã© composto
principalmente por paÃ­ses de lÃ­ngua inglesa, sendo os seguintes os paÃ­ses atualmente disponÃ­veis:

> Australia, Austria, Canada, England, France, Germany, Ireland, Isle of Man, Italy,
> Japan, Poland, Portugal, Scotland, Singapore, South Africa, South Korea, Sweden,
> Ukraine e United States.

**PaÃ­ses como Brasil nÃ£o estÃ£o presentes na fonte de dados.** Isso nÃ£o Ã© uma limitaÃ§Ã£o
do pipeline, mas sim da origem dos dados. O pipeline processa corretamente todos os
registros disponibilizados pela API.

### ExtensÃ£o sugerida para outras fontes

Para cobrir mercados nÃ£o presentes na Open Brewery DB (como o Brasil), o pipeline
poderia ser estendido para ingerir dados de fontes complementares, como:

- **Untappd API** â€” base de dados global de cervejas e cervejarias
- **RateBeer API** â€” catÃ¡logo global com cobertura de paÃ­ses da AmÃ©rica Latina
- **Scraping de associaÃ§Ãµes locais** â€” ex: CervBrasil (AssociaÃ§Ã£o Brasileira de Cerveja Artesanal)

A arquitetura Medallion adotada facilita essa extensÃ£o: bastaria adicionar novos
operadores na camada Bronze para cada fonte adicional, mantendo as camadas Silver
e Gold agnÃ³sticas Ã  origem dos dados.
